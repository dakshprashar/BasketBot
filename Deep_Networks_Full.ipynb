{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:32:18.038935: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import parse_data\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Feature and Response Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = parse_data.df_dic\n",
    "\n",
    "def make_X_and_y(df_dic):\n",
    "    X_dic = {}\n",
    "    y_dic = {}\n",
    "    for k in df_dic:\n",
    "        df = df_dic[k]\n",
    "        cols_X = [0,1,2,3,4,5,7,8,9,10]\n",
    "        cols_y = [6]\n",
    "        df_X = df.iloc[:, cols_X].copy()\n",
    "        df_y = df.iloc[:, cols_y].copy()\n",
    "        X_dic[k] = df_X.values.tolist()\n",
    "        y_dic[k] = df_y.values.tolist()\n",
    "    return X_dic, y_dic\n",
    "\n",
    "X_dic, y_dic = make_X_and_y(df_dic)\n",
    "\n",
    "X = np.array(sum(X_dic.values(), []))\n",
    "y = np.array(sum(y_dic.values(), [])).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Neural Network Model using 5-Fold Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model():\n",
    "    nn_model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dense(64, activation='relu'),  # Input Layer\n",
    "        keras.layers.Dense(32, activation='relu'),  # Hidden Layer 1\n",
    "        keras.layers.Dense(16, activation='relu'),  # Hidden Layer 2\n",
    "        keras.layers.Dense(1)  # Output Layer (Regression, 1 Neuron)\n",
    "    ])\n",
    "    nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    return nn_model\n",
    "\n",
    "k = 5 \n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "nn_models = []\n",
    "num_models = 0\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_temp_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_temp_fold = y_train[train_index], y_train[val_index]\n",
    "    n = len(X_temp_fold)\n",
    "    X_val_fold, X_test_fold = X_temp_fold[:n//2], X_temp_fold[n//2:]\n",
    "    y_val_fold, y_test_fold = y_temp_fold[:n//2], y_temp_fold[n//2:]\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_nn_model()\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, validation_data=(X_val_fold, y_val_fold))\n",
    "    nn_models.append(model)\n",
    "\n",
    "    # Evaluate the model on validation fold\n",
    "    loss, mae = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
    "\n",
    "    mse_scores.append(loss) \n",
    "    mae_scores.append(mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I tried getting the average weights of all CV model to then fit an aggregated model but that gave me very general results that were not useful.\n",
    "Got better results when I averaged over predictions for each model that was fit in the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 161.3601 ± 4.3378\n",
      "Average MAE: 9.9763 ± 0.1321\n"
     ]
    }
   ],
   "source": [
    "## 16m 23s runtime,\n",
    "print(f\"Average MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results w/ No k fold CV: MSE = 170\n",
    "\n",
    "Results w/ k fold CV: MSE = 161.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Fitting a XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best RMSE: 12.6653\n",
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "## XGBOOST - Good for tabular data, Handles linearity & interactions b/w features automatically.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", eval_metric=\"rmse\", random_state=12)\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 300], # Number of boosting rounds (trees)\n",
    "    \"max_depth\": [3, 6, 9], # Depth of trees\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],  # % of the data per tree\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0] # % of the features per tree\n",
    "}\n",
    "\n",
    "## Finding the best hyperparameter values to fit model on\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=10, scoring=\"neg_root_mean_squared_error\", cv=5, verbose=1, random_state=42)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_rmse = -search.best_score_\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(\"Best Parameters:\", search.best_params_)\n",
    "\n",
    "best_xg_model = search.best_estimator_\n",
    "## 1m 48s runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Sequential name=sequential, built=True>, <Sequential name=sequential_1, built=True>, <Sequential name=sequential_2, built=True>, <Sequential name=sequential_3, built=True>, <Sequential name=sequential_4, built=True>]\n"
     ]
    }
   ],
   "source": [
    "print(nn_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Neural Net Prediction: -2.1129336\n",
      "XGBoost Prediction: -3.2927372\n",
      "Actual Spread: -8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, len(y) - 1)\n",
    "\n",
    "X_i = X[i]\n",
    "X_i = np.array(X_i).reshape(1, -1) \n",
    "y_i = y[i]\n",
    "\n",
    "## Neural Network Prediction\n",
    "def nn_predict(models, X_i):\n",
    "    X_i = np.array(X_i).reshape(1, -1)  # Ensure input shape is (1, 10)\n",
    "    preds = np.array([model.predict(X_i) for model in models])\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "nn_spread = nn_predict(nn_models, X_i)[0][0]\n",
    "\n",
    "## XGBoost Prediction\n",
    "xg_spread = best_xg_model.predict(X_i)[0]\n",
    "\n",
    "print(\"Neural Net Prediction:\", nn_spread)\n",
    "print(\"XGBoost Prediction:\", xg_spread)\n",
    "print(\"Actual Spread:\", y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the XGBoost model is giving us much better results and is more efficient while training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
